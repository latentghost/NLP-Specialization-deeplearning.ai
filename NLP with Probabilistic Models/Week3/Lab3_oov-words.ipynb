{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "m = 3\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
    "vocab = [word[0] for word in Counter(word_counts).most_common(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'because', 'learning']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentence\n",
    "sentence = ['am', 'i', 'learning']\n",
    "output_sentence = []\n",
    "\n",
    "# Output sentence\n",
    "for w in sentence:\n",
    "    if w not in vocab:\n",
    "        output_sentence.append(\"<UNK>\")\n",
    "    else:\n",
    "        output_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', '<UNK>', 'learning']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity without <UNK>:\t1.2599210498948732\n",
      "Perplexity with <UNK>:\t\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Large # of <UNK>s can cause bad performance\n",
    "# Following is an example of this\n",
    "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
    "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
    "\n",
    "test_set = ['i', 'am', 'learning']\n",
    "test_set_unk = ['i', 'am', '<UNK>']\n",
    "\n",
    "M = len(test_set)\n",
    "probability = 1\n",
    "probability_unk = 1\n",
    "\n",
    "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
    "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
    "\n",
    "for i in range(len(test_set)-1):\n",
    "    bigram = tuple(test_set[i:i+2])\n",
    "    probability *= bigram_probabilities[bigram]\n",
    "\n",
    "    bigram_unk = tuple(test_set_unk[i:i+2])\n",
    "    probability_unk *= bigram_probabilities_unk[bigram_unk]\n",
    "\n",
    "perplexity = probability**(-1/M)\n",
    "perplexity_unk = probability_unk**(-1/M)\n",
    "\n",
    "print(f\"Perplexity without <UNK>:\\t{perplexity}\")\n",
    "print(f\"Perplexity with <UNK>:\\t\\t{perplexity_unk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing\n",
    "def add_k_smoothing(k,vocab_size,n_gram_count,n_gram_pref_count):\n",
    "    num = n_gram_count + k\n",
    "    den = n_gram_pref_count + k*vocab_size\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability_known_trigram:\t0.2\n",
      "probability_unknown_trigram:\t0.2\n"
     ]
    }
   ],
   "source": [
    "# N-grams not seen in the corpus get veru high probabilities\n",
    "trigram_probabilities = {('i', 'am', 'happy') : 2}\n",
    "bigram_probabilities = {( 'i', 'am') : 10}\n",
    "vocabulary_size = 5\n",
    "k = 1\n",
    "\n",
    "probability_known_trigram = add_k_smoothing(k,vocabulary_size,trigram_probabilities[('i', 'am', 'happy')],bigram_probabilities[( 'i', 'am')])\n",
    "probability_unknown_trigram = add_k_smoothing(k,vocabulary_size,0,0)\n",
    "\n",
    "print(f\"probability_known_trigram:\\t{probability_known_trigram}\")\n",
    "print(f\"probability_unknown_trigram:\\t{probability_unknown_trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: ('are', 'you', 'happy')\n",
      "Bigram: ('you', 'happy')\n",
      "Unigram: happy\n",
      "\n",
      "Trigram ('are', 'you', 'happy') not found\n",
      "Bigram ('you', 'happy') not found\n",
      "Unigram happy found\n",
      "\n",
      "P(('are', 'you', 'happy')) estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "source": [
    "# Backoff\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# Trigram to estimate\n",
    "trigram = ('are', 'you', 'happy')\n",
    "\n",
    "# All preceeding lower order N-grams\n",
    "bigram = trigram[1:3]\n",
    "unigram = trigram[2]\n",
    "print(f\"Trigram: {trigram}\\nBigram: {bigram}\\nUnigram: {unigram}\\n\")\n",
    "\n",
    "# Stupid Backoff Constant\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# Search for first non-zero probability starting with N-gram\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram]==0:\n",
    "    print(f\"Trigram {trigram} not found\")\n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram]==0:\n",
    "        print(f\"Bigram {bigram} not found\")\n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"Unigram {unigram} found\\n\")\n",
    "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"P({trigram}) estimated as {probability_hat_trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(('i', 'am', 'happy')) estimated as 0.12\n"
     ]
    }
   ],
   "source": [
    "# Interpolation --> Linear Weighted Sum of all lower order N-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# Weights learnt from val set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# Input Trigram to estimate\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# Lower order N-grams of the input\n",
    "bigram = trigram[1:3]\n",
    "unigram = trigram[2]\n",
    "\n",
    "# Assuming all are present, else 0\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities.get(trigram,0)\n",
    "+ lambda_2 * bigram_probabilities.get(bigram,0)\n",
    "+ lambda_3 * unigram_probabilities.get(unigram,0)\n",
    "\n",
    "print(f\"P({trigram}) estimated as {probability_hat_trigram}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
