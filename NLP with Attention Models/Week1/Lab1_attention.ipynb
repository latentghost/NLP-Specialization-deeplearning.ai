{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Attention operation as described in [Bhadanau, et al (2014)](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=0):\n",
    "    \"\"\" Calculate softmax function for an array x along specified axis\n",
    "    \n",
    "        axis=0 calculates softmax across rows which means each column sums to 1 \n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "attention_size = 10\n",
    "input_length = 5\n",
    "np.random.seed(42)\n",
    "\n",
    "# Encoder states (1 for each input)\n",
    "encoder_states = np.random.randn(input_length, hidden_size)\n",
    "\n",
    "# Previous Decoder Hidden State\n",
    "decoder_state = np.random.randn(1, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for the Neural Network\n",
    "layer_1 = np.random.randn(2 * hidden_size, attention_size)\n",
    "layer_2 = np.random.randn(attention_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment(encoder_states,decoder_state):\n",
    "    # Concatenate the encoder states and the decoder state, decoder state is concatenated with each hidden state individually\n",
    "    inputs = np.concatenate([encoder_states,decoder_state.repeat(repeats=encoder_states.shape[0],axis=0)],axis=1)\n",
    "    assert inputs.shape == (input_length, 2 * hidden_size)\n",
    "\n",
    "    # Matrix multiplication of the concatenated inputs and layer_1, with tanh activation\n",
    "    activations = np.tanh(inputs@layer_1)\n",
    "    assert activations.shape == (input_length, attention_size)\n",
    "\n",
    "    # Matrix multiplication of the activations with layer_2\n",
    "    scores = activations@layer_2\n",
    "    assert scores.shape == (input_length, 1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35790943]\n",
      " [5.92373433]\n",
      " [4.18673175]\n",
      " [2.11437202]\n",
      " [0.95767155]]\n"
     ]
    }
   ],
   "source": [
    "scores = alignment(encoder_states, decoder_state)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Alignment Scores to Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(encoder_states, decoder_state):\n",
    "    \"\"\" Example function that calculates attention, returns the context vector \n",
    "    \n",
    "        Arguments:\n",
    "        encoder_vectors: NxM numpy array, where N is the number of vectors and M is the vector length\n",
    "        decoder_vector: 1xM numpy array, M is the vector length, much be the same M as encoder_vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate alignment scores\n",
    "    scores = alignment(encoder_states,decoder_state)\n",
    "\n",
    "    # Convert to weights\n",
    "    weights = softmax(scores,axis=0)\n",
    "\n",
    "    # Multiply each encoder state by its respective weight\n",
    "    weighted_scores = weights*encoder_states\n",
    "\n",
    "    # Sum up weighted alignment vectors\n",
    "    context = np.sum(weighted_scores,axis=0)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n",
      "  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n",
      " -0.58015282 -0.58294027 -0.75457577  1.32985756]\n"
     ]
    }
   ],
   "source": [
    "context_vector = attention(encoder_states, decoder_state)\n",
    "print(context_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
